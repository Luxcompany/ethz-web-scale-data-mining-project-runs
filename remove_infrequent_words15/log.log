Spark assembly has been built with Hive, including Datanucleus jars on classpath
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0
=====================================
Remove Infrequent Words Configuration
-------------------------------------
maxWordCount=2147483647, minWordCount=20000, output=hdfs://dco-node121.dco.ethz.ch:54310/cw-combined-pruned-20000, inputCombined=hdfs://dco-node121.dco.ethz.ch:54310/cw-combined, inputWordcount=hdfs://dco-node121.dco.ethz.ch:54310/cw-wordcount/wordcounts.txt
=====================================
2014-07-23 06:02:34,050 [main] WARN  org.apache.spark.SparkConf - 
SPARK_JAVA_OPTS was detected (set to '-Xms4g -Xmx70g  -Dspark.akka.frameSize=2000').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
2014-07-23 06:02:34,052 [main] WARN  org.apache.spark.SparkConf - Setting 'spark.executor.extraJavaOptions' to '-Xms4g -Xmx70g  -Dspark.akka.frameSize=2000' as a work-around.
2014-07-23 06:02:34,052 [main] WARN  org.apache.spark.SparkConf - Setting 'spark.driver.extraJavaOptions' to '-Xms4g -Xmx70g  -Dspark.akka.frameSize=2000' as a work-around.
2014-07-23 06:02:34,106 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: root
2014-07-23 06:02:34,107 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root)
2014-07-23 06:02:34,545 [spark-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-07-23 06:02:34,603 [spark-akka.actor.default-dispatcher-6] INFO  Remoting - Starting remoting
2014-07-23 06:02:34,786 [spark-akka.actor.default-dispatcher-6] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://spark@dco-node121-mgt.dco.ethz.ch:56275]
2014-07-23 06:02:34,791 [spark-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting now listens on addresses: [akka.tcp://spark@dco-node121-mgt.dco.ethz.ch:56275]
2014-07-23 06:02:34,824 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2014-07-23 06:02:34,827 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2014-07-23 06:02:34,849 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/spark-local-20140723060234-7a86
2014-07-23 06:02:34,854 [main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 40.3 GB.
2014-07-23 06:02:34,883 [main] INFO  org.apache.spark.network.ConnectionManager - Bound socket to port 48754 with id = ConnectionManagerId(dco-node121-mgt.dco.ethz.ch,48754)
2014-07-23 06:02:34,888 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2014-07-23 06:02:34,891 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node121-mgt.dco.ethz.ch:48754 with 40.3 GB RAM
2014-07-23 06:02:34,892 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2014-07-23 06:02:34,909 [main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2014-07-23 06:02:34,966 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 06:02:34,986 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:37244
2014-07-23 06:02:34,986 [main] INFO  org.apache.spark.broadcast.HttpBroadcast - Broadcast server started at http://172.31.109.131:37244
2014-07-23 06:02:34,992 [main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /tmp/spark-7397ec2d-a671-4259-bc21-b02c9ab09f2c
2014-07-23 06:02:34,993 [main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2014-07-23 06:02:34,993 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 06:02:34,996 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:44851
2014-07-23 06:02:35,342 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 06:02:35,355 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,360 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED org.eclipse.jetty.server.Server@10ded6a9: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,364 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
2014-07-23 06:02:35,365 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2014-07-23 06:02:35,365 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2014-07-23 06:02:35,365 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2014-07-23 06:02:35,366 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2014-07-23 06:02:35,366 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2014-07-23 06:02:35,366 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2014-07-23 06:02:35,367 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2014-07-23 06:02:35,367 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2014-07-23 06:02:35,367 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2014-07-23 06:02:35,367 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2014-07-23 06:02:35,368 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2014-07-23 06:02:35,368 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2014-07-23 06:02:35,368 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2014-07-23 06:02:35,368 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2014-07-23 06:02:35,369 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2014-07-23 06:02:35,369 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2014-07-23 06:02:35,369 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2014-07-23 06:02:35,423 [main] INFO  org.apache.spark.ui.JettyUtils - Failed to create UI at port, 4040. Trying again.
2014-07-23 06:02:35,424 [main] INFO  org.apache.spark.ui.JettyUtils - Error was: Failure(java.net.BindException: Address already in use)
2014-07-23 06:02:35,427 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 06:02:35,438 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED SelectChannelConnector@0.0.0.0:4041: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,439 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED org.eclipse.jetty.server.Server@2ceb80a1: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,441 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
2014-07-23 06:02:35,441 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2014-07-23 06:02:35,441 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2014-07-23 06:02:35,442 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2014-07-23 06:02:35,442 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2014-07-23 06:02:35,442 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2014-07-23 06:02:35,442 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2014-07-23 06:02:35,442 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2014-07-23 06:02:35,442 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2014-07-23 06:02:35,443 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2014-07-23 06:02:35,443 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2014-07-23 06:02:35,443 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2014-07-23 06:02:35,443 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2014-07-23 06:02:35,443 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2014-07-23 06:02:35,444 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2014-07-23 06:02:35,444 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2014-07-23 06:02:35,444 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2014-07-23 06:02:35,444 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2014-07-23 06:02:35,499 [main] INFO  org.apache.spark.ui.JettyUtils - Failed to create UI at port, 4041. Trying again.
2014-07-23 06:02:35,500 [main] INFO  org.apache.spark.ui.JettyUtils - Error was: Failure(java.net.BindException: Address already in use)
2014-07-23 06:02:35,502 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 06:02:35,511 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED SelectChannelConnector@0.0.0.0:4042: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,513 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED org.eclipse.jetty.server.Server@340da44c: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,514 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
2014-07-23 06:02:35,514 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2014-07-23 06:02:35,515 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2014-07-23 06:02:35,515 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2014-07-23 06:02:35,515 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2014-07-23 06:02:35,516 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2014-07-23 06:02:35,516 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2014-07-23 06:02:35,517 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2014-07-23 06:02:35,517 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2014-07-23 06:02:35,517 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2014-07-23 06:02:35,518 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2014-07-23 06:02:35,518 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2014-07-23 06:02:35,518 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2014-07-23 06:02:35,518 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2014-07-23 06:02:35,518 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2014-07-23 06:02:35,518 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2014-07-23 06:02:35,519 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2014-07-23 06:02:35,519 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2014-07-23 06:02:35,574 [main] INFO  org.apache.spark.ui.JettyUtils - Failed to create UI at port, 4042. Trying again.
2014-07-23 06:02:35,574 [main] INFO  org.apache.spark.ui.JettyUtils - Error was: Failure(java.net.BindException: Address already in use)
2014-07-23 06:02:35,577 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 06:02:35,584 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED SelectChannelConnector@0.0.0.0:4043: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,585 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED org.eclipse.jetty.server.Server@119020fb: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,586 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
2014-07-23 06:02:35,587 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2014-07-23 06:02:35,587 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2014-07-23 06:02:35,587 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2014-07-23 06:02:35,588 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2014-07-23 06:02:35,588 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2014-07-23 06:02:35,588 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2014-07-23 06:02:35,588 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2014-07-23 06:02:35,588 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2014-07-23 06:02:35,588 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2014-07-23 06:02:35,589 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2014-07-23 06:02:35,589 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2014-07-23 06:02:35,589 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2014-07-23 06:02:35,589 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2014-07-23 06:02:35,589 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2014-07-23 06:02:35,590 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2014-07-23 06:02:35,590 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2014-07-23 06:02:35,590 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2014-07-23 06:02:35,644 [main] INFO  org.apache.spark.ui.JettyUtils - Failed to create UI at port, 4043. Trying again.
2014-07-23 06:02:35,644 [main] INFO  org.apache.spark.ui.JettyUtils - Error was: Failure(java.net.BindException: Address already in use)
2014-07-23 06:02:35,647 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 06:02:35,653 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED SelectChannelConnector@0.0.0.0:4044: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,655 [main] WARN  org.eclipse.jetty.util.component.AbstractLifeCycle - FAILED org.eclipse.jetty.server.Server@6c5945a7: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:293)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply$mcV$sp(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at org.apache.spark.ui.JettyUtils$$anonfun$1.apply(JettyUtils.scala:192)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.ui.JettyUtils$.connect$1(JettyUtils.scala:191)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:205)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:99)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:223)
	at RemoveInfrequentWordsApp$.createSparkContext(RemoveInfrequentWordsApp.scala:98)
	at RemoveInfrequentWordsApp$.main(RemoveInfrequentWordsApp.scala:15)
	at RemoveInfrequentWordsApp.main(RemoveInfrequentWordsApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:292)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
2014-07-23 06:02:35,656 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
2014-07-23 06:02:35,657 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
2014-07-23 06:02:35,657 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/,null}
2014-07-23 06:02:35,657 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/static,null}
2014-07-23 06:02:35,658 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors/json,null}
2014-07-23 06:02:35,658 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/executors,null}
2014-07-23 06:02:35,658 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment/json,null}
2014-07-23 06:02:35,658 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/environment,null}
2014-07-23 06:02:35,659 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
2014-07-23 06:02:35,659 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
2014-07-23 06:02:35,659 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage/json,null}
2014-07-23 06:02:35,659 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/storage,null}
2014-07-23 06:02:35,660 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
2014-07-23 06:02:35,660 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
2014-07-23 06:02:35,660 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
2014-07-23 06:02:35,660 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
2014-07-23 06:02:35,660 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages/json,null}
2014-07-23 06:02:35,661 [main] INFO  org.eclipse.jetty.server.handler.ContextHandler - stopped o.e.j.s.ServletContextHandler{/stages,null}
2014-07-23 06:02:35,715 [main] INFO  org.apache.spark.ui.JettyUtils - Failed to create UI at port, 4044. Trying again.
2014-07-23 06:02:35,716 [main] INFO  org.apache.spark.ui.JettyUtils - Error was: Failure(java.net.BindException: Address already in use)
2014-07-23 06:02:35,718 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 06:02:35,733 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4045
2014-07-23 06:02:35,738 [main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://dco-node121-mgt.dco.ethz.ch:4045
2014-07-23 06:02:35,965 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2014-07-23 06:02:36,601 [main] INFO  org.apache.spark.scheduler.EventLoggingListener - Logging events to hdfs://dco-node121.dco.ethz.ch:54310/spark_event_log/remove-infrequent-words-1406088156062
2014-07-23 06:02:36,825 [main] INFO  org.apache.spark.SparkContext - Added JAR file:/disk3/user_work/runs/remove_infrequent_words15/run.jar at http://172.31.109.131:44851/jars/run.jar with timestamp 1406088156824
2014-07-23 06:02:36,890 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Connecting to master spark://dco-node121.dco.ethz.ch:7077...
2014-07-23 06:02:37,187 [main] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(231905) called with curMem=0, maxMem=43218213273
2014-07-23 06:02:37,190 [main] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values to memory (estimated size 226.5 KB, free 40.2 GB)
2014-07-23 06:02:37,244 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Connected to Spark cluster with app ID app-20140723060237-0018
2014-07-23 06:02:37,464 [main] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2014-07-23 06:02:37,511 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.146:50010
2014-07-23 06:02:37,512 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.135:50010
2014-07-23 06:02:37,548 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.133:50010
2014-07-23 06:02:37,548 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.139:50010
2014-07-23 06:02:37,548 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.142:50010
2014-07-23 06:02:37,558 [main] INFO  org.apache.spark.SparkContext - Starting job: collect at RemoveInfrequentWordsApp.scala:51
2014-07-23 06:02:37,580 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (collect at RemoveInfrequentWordsApp.scala:51) with 81 output partitions (allowLocal=false)
2014-07-23 06:02:37,581 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(collect at RemoveInfrequentWordsApp.scala:51)
2014-07-23 06:02:37,581 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2014-07-23 06:02:37,613 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2014-07-23 06:02:37,621 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 0 (FilteredRDD[5] at filter at RemoveInfrequentWordsApp.scala:50), which has no missing parents
2014-07-23 06:02:37,693 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 81 missing tasks from Stage 0 (FilteredRDD[5] at filter at RemoveInfrequentWordsApp.scala:50)
2014-07-23 06:02:37,696 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 81 tasks
2014-07-23 06:02:52,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:03:07,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:03:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:03:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:03:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:04:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:04:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:04:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:04:52,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:05:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:05:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:05:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:05:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:06:07,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:06:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:06:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:06:52,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:07:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:07:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:07:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:07:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:08:07,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:08:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:08:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:08:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:09:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:09:22,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:09:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:09:52,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:10:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:10:22,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:10:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:10:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:11:07,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:11:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:11:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:11:52,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:12:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:12:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:12:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:12:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:13:07,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:13:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:13:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:13:52,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:14:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:14:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:14:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:14:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:15:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:15:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:15:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:15:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:16:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:16:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:16:37,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:16:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:17:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:17:22,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:17:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:17:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:18:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:18:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:18:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:18:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:19:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:19:22,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:19:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:19:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:20:07,707 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:20:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:20:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:20:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:21:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:21:22,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:21:37,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:21:52,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:22:07,706 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 06:22:18,987 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/0 on worker-20140722232232-dco-node135-mgt.dco.ethz.ch-56045 (dco-node135-mgt.dco.ethz.ch:56045) with 2 cores
2014-07-23 06:22:18,989 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/0 on hostPort dco-node135-mgt.dco.ethz.ch:56045 with 2 cores, 70.0 GB RAM
2014-07-23 06:22:18,989 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/1 on worker-20140722232232-dco-node125-mgt.dco.ethz.ch-41556 (dco-node125-mgt.dco.ethz.ch:41556) with 2 cores
2014-07-23 06:22:18,990 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/1 on hostPort dco-node125-mgt.dco.ethz.ch:41556 with 2 cores, 70.0 GB RAM
2014-07-23 06:22:18,990 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/2 on worker-20140722232232-dco-node124-mgt.dco.ethz.ch-46656 (dco-node124-mgt.dco.ethz.ch:46656) with 2 cores
2014-07-23 06:22:18,991 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/2 on hostPort dco-node124-mgt.dco.ethz.ch:46656 with 2 cores, 70.0 GB RAM
2014-07-23 06:22:18,991 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/3 on worker-20140722232232-dco-node123-mgt.dco.ethz.ch-52179 (dco-node123-mgt.dco.ethz.ch:52179) with 2 cores
2014-07-23 06:22:18,992 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/3 on hostPort dco-node123-mgt.dco.ethz.ch:52179 with 2 cores, 70.0 GB RAM
2014-07-23 06:22:18,992 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/4 on worker-20140722232232-dco-node122-mgt.dco.ethz.ch-51038 (dco-node122-mgt.dco.ethz.ch:51038) with 1 cores
2014-07-23 06:22:18,993 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/4 on hostPort dco-node122-mgt.dco.ethz.ch:51038 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:18,993 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/5 on worker-20140722232232-dco-node132-mgt.dco.ethz.ch-45072 (dco-node132-mgt.dco.ethz.ch:45072) with 1 cores
2014-07-23 06:22:18,994 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/5 on hostPort dco-node132-mgt.dco.ethz.ch:45072 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:18,995 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/6 on worker-20140722232232-dco-node131-mgt.dco.ethz.ch-50734 (dco-node131-mgt.dco.ethz.ch:50734) with 1 cores
2014-07-23 06:22:18,996 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/6 on hostPort dco-node131-mgt.dco.ethz.ch:50734 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:18,996 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/7 on worker-20140722232232-dco-node133-mgt.dco.ethz.ch-44156 (dco-node133-mgt.dco.ethz.ch:44156) with 1 cores
2014-07-23 06:22:18,997 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/7 on hostPort dco-node133-mgt.dco.ethz.ch:44156 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:18,998 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/8 on worker-20140722232232-dco-node127-mgt.dco.ethz.ch-60499 (dco-node127-mgt.dco.ethz.ch:60499) with 1 cores
2014-07-23 06:22:18,999 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/8 on hostPort dco-node127-mgt.dco.ethz.ch:60499 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:18,999 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/9 on worker-20140722232232-dco-node121-mgt.dco.ethz.ch-45820 (dco-node121-mgt.dco.ethz.ch:45820) with 1 cores
2014-07-23 06:22:19,000 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/9 on hostPort dco-node121-mgt.dco.ethz.ch:45820 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:19,000 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/10 on worker-20140722232232-dco-node129-mgt.dco.ethz.ch-37573 (dco-node129-mgt.dco.ethz.ch:37573) with 1 cores
2014-07-23 06:22:19,006 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/10 on hostPort dco-node129-mgt.dco.ethz.ch:37573 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:19,006 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/11 on worker-20140722232232-dco-node128-mgt.dco.ethz.ch-58779 (dco-node128-mgt.dco.ethz.ch:58779) with 1 cores
2014-07-23 06:22:19,007 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/11 on hostPort dco-node128-mgt.dco.ethz.ch:58779 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:19,007 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/12 on worker-20140722232232-dco-node134-mgt.dco.ethz.ch-42367 (dco-node134-mgt.dco.ethz.ch:42367) with 1 cores
2014-07-23 06:22:19,010 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/12 on hostPort dco-node134-mgt.dco.ethz.ch:42367 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:19,010 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/13 on worker-20140722232231-dco-node130-mgt.dco.ethz.ch-56551 (dco-node130-mgt.dco.ethz.ch:56551) with 1 cores
2014-07-23 06:22:19,011 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/13 on hostPort dco-node130-mgt.dco.ethz.ch:56551 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:19,011 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/14 on worker-20140722232232-dco-node136-mgt.dco.ethz.ch-34960 (dco-node136-mgt.dco.ethz.ch:34960) with 1 cores
2014-07-23 06:22:19,012 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/14 on hostPort dco-node136-mgt.dco.ethz.ch:34960 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:19,012 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor added: app-20140723060237-0018/15 on worker-20140722232232-dco-node126-mgt.dco.ethz.ch-38949 (dco-node126-mgt.dco.ethz.ch:38949) with 1 cores
2014-07-23 06:22:19,012 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Granted executor ID app-20140723060237-0018/15 on hostPort dco-node126-mgt.dco.ethz.ch:38949 with 1 cores, 70.0 GB RAM
2014-07-23 06:22:19,019 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/9 is now RUNNING
2014-07-23 06:22:19,020 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/14 is now RUNNING
2014-07-23 06:22:19,022 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/5 is now RUNNING
2014-07-23 06:22:19,025 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/12 is now RUNNING
2014-07-23 06:22:19,027 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/10 is now RUNNING
2014-07-23 06:22:19,029 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/1 is now RUNNING
2014-07-23 06:22:19,032 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/15 is now RUNNING
2014-07-23 06:22:19,034 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/2 is now RUNNING
2014-07-23 06:22:19,037 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/11 is now RUNNING
2014-07-23 06:22:19,038 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/4 is now RUNNING
2014-07-23 06:22:19,042 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/8 is now RUNNING
2014-07-23 06:22:19,044 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/13 is now RUNNING
2014-07-23 06:22:19,047 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/0 is now RUNNING
2014-07-23 06:22:19,049 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/3 is now RUNNING
2014-07-23 06:22:19,051 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/6 is now RUNNING
2014-07-23 06:22:19,093 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Executor updated: app-20140723060237-0018/7 is now RUNNING
2014-07-23 06:22:21,095 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node130-mgt.dco.ethz.ch:46964/user/Executor#-2128397190] with ID 13
2014-07-23 06:22:21,104 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:0 as TID 0 on executor 13: dco-node130-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:21,109 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:0 as 2027 bytes in 4 ms
2014-07-23 06:22:21,369 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node130-mgt.dco.ethz.ch:34871 with 40.3 GB RAM
2014-07-23 06:22:22,040 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node129-mgt.dco.ethz.ch:42387/user/Executor#-1583757156] with ID 10
2014-07-23 06:22:22,041 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:1 as TID 1 on executor 10: dco-node129-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,043 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:1 as 2027 bytes in 1 ms
2014-07-23 06:22:22,059 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node123-mgt.dco.ethz.ch:38784/user/Executor#2111185229] with ID 3
2014-07-23 06:22:22,065 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:2 as TID 2 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,066 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:2 as 2027 bytes in 0 ms
2014-07-23 06:22:22,066 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:3 as TID 3 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,067 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:3 as 2027 bytes in 1 ms
2014-07-23 06:22:22,067 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node132-mgt.dco.ethz.ch:44611/user/Executor#186468420] with ID 5
2014-07-23 06:22:22,068 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:4 as TID 4 on executor 5: dco-node132-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,068 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:4 as 2027 bytes in 0 ms
2014-07-23 06:22:22,070 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node127-mgt.dco.ethz.ch:41294/user/Executor#1311092093] with ID 8
2014-07-23 06:22:22,070 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:5 as TID 5 on executor 8: dco-node127-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,071 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:5 as 2027 bytes in 0 ms
2014-07-23 06:22:22,080 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node125-mgt.dco.ethz.ch:54321/user/Executor#1494813422] with ID 1
2014-07-23 06:22:22,081 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:6 as TID 6 on executor 1: dco-node125-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,081 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:6 as 2027 bytes in 0 ms
2014-07-23 06:22:22,081 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:7 as TID 7 on executor 1: dco-node125-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,082 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:7 as 2027 bytes in 1 ms
2014-07-23 06:22:22,090 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node126-mgt.dco.ethz.ch:37595/user/Executor#1312464006] with ID 15
2014-07-23 06:22:22,091 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:8 as TID 8 on executor 15: dco-node126-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,092 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:8 as 2027 bytes in 1 ms
2014-07-23 06:22:22,093 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node136-mgt.dco.ethz.ch:53228/user/Executor#-1892027326] with ID 14
2014-07-23 06:22:22,094 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:9 as TID 9 on executor 14: dco-node136-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,095 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:9 as 2027 bytes in 0 ms
2014-07-23 06:22:22,102 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node128-mgt.dco.ethz.ch:35190/user/Executor#-1524062362] with ID 11
2014-07-23 06:22:22,103 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:10 as TID 10 on executor 11: dco-node128-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,104 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:10 as 2027 bytes in 1 ms
2014-07-23 06:22:22,107 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node122-mgt.dco.ethz.ch:55809/user/Executor#-499587622] with ID 4
2014-07-23 06:22:22,108 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:11 as TID 11 on executor 4: dco-node122-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,109 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:11 as 2027 bytes in 1 ms
2014-07-23 06:22:22,110 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node121-mgt.dco.ethz.ch:51310/user/Executor#-289354089] with ID 9
2014-07-23 06:22:22,111 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:12 as TID 12 on executor 9: dco-node121-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,112 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:12 as 2027 bytes in 0 ms
2014-07-23 06:22:22,113 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node124-mgt.dco.ethz.ch:50137/user/Executor#-485813887] with ID 2
2014-07-23 06:22:22,115 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:13 as TID 13 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,116 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:13 as 2027 bytes in 1 ms
2014-07-23 06:22:22,116 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:14 as TID 14 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,117 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:14 as 2027 bytes in 1 ms
2014-07-23 06:22:22,120 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node131-mgt.dco.ethz.ch:45344/user/Executor#1286531889] with ID 6
2014-07-23 06:22:22,121 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:15 as TID 15 on executor 6: dco-node131-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,121 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:15 as 2027 bytes in 0 ms
2014-07-23 06:22:22,241 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node134-mgt.dco.ethz.ch:50684/user/Executor#1089677436] with ID 12
2014-07-23 06:22:22,243 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:16 as TID 16 on executor 12: dco-node134-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,243 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:16 as 2027 bytes in 0 ms
2014-07-23 06:22:22,316 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node127-mgt.dco.ethz.ch:57396 with 40.3 GB RAM
2014-07-23 06:22:22,325 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node132-mgt.dco.ethz.ch:60604 with 40.3 GB RAM
2014-07-23 06:22:22,339 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node123-mgt.dco.ethz.ch:36908 with 40.3 GB RAM
2014-07-23 06:22:22,343 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node125-mgt.dco.ethz.ch:60482 with 40.3 GB RAM
2014-07-23 06:22:22,348 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node135-mgt.dco.ethz.ch:39171/user/Executor#-1768278559] with ID 0
2014-07-23 06:22:22,349 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:17 as TID 17 on executor 0: dco-node135-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,349 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:17 as 2027 bytes in 0 ms
2014-07-23 06:22:22,350 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:18 as TID 18 on executor 0: dco-node135-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,350 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:18 as 2027 bytes in 0 ms
2014-07-23 06:22:22,358 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node126-mgt.dco.ethz.ch:45961 with 40.3 GB RAM
2014-07-23 06:22:22,361 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node124-mgt.dco.ethz.ch:55315 with 40.3 GB RAM
2014-07-23 06:22:22,365 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node136-mgt.dco.ethz.ch:59023 with 40.3 GB RAM
2014-07-23 06:22:22,370 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node128-mgt.dco.ethz.ch:46277 with 40.3 GB RAM
2014-07-23 06:22:22,384 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node131-mgt.dco.ethz.ch:53302 with 40.3 GB RAM
2014-07-23 06:22:22,390 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node122-mgt.dco.ethz.ch:47915 with 40.3 GB RAM
2014-07-23 06:22:22,390 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node121-mgt.dco.ethz.ch:46661 with 40.3 GB RAM
2014-07-23 06:22:22,503 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node134-mgt.dco.ethz.ch:51994 with 40.3 GB RAM
2014-07-23 06:22:22,563 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Registered executor: Actor[akka.tcp://sparkExecutor@dco-node133-mgt.dco.ethz.ch:44967/user/Executor#-1087767241] with ID 7
2014-07-23 06:22:22,564 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:19 as TID 19 on executor 7: dco-node133-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:22,565 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:19 as 2027 bytes in 0 ms
2014-07-23 06:22:22,597 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node135-mgt.dco.ethz.ch:41418 with 40.3 GB RAM
2014-07-23 06:22:22,639 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node129-mgt.dco.ethz.ch:44893 with 40.3 GB RAM
2014-07-23 06:22:22,813 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node133-mgt.dco.ethz.ch:36115 with 40.3 GB RAM
2014-07-23 06:22:25,725 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:20 as TID 20 on executor 13: dco-node130-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:25,726 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:20 as 2027 bytes in 0 ms
2014-07-23 06:22:25,758 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 0 in 4637 ms on dco-node130-mgt.dco.ethz.ch (progress: 1/81)
2014-07-23 06:22:25,759 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 0)
2014-07-23 06:22:25,788 [spark-akka.actor.default-dispatcher-19] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:21 as TID 21 on executor 9: dco-node121-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:25,788 [spark-akka.actor.default-dispatcher-19] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:21 as 2027 bytes in 0 ms
2014-07-23 06:22:25,795 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 12 in 3678 ms on dco-node121-mgt.dco.ethz.ch (progress: 2/81)
2014-07-23 06:22:25,795 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 12)
2014-07-23 06:22:27,033 [spark-akka.actor.default-dispatcher-19] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:22 as TID 22 on executor 13: dco-node130-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:27,033 [spark-akka.actor.default-dispatcher-19] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:22 as 2027 bytes in 0 ms
2014-07-23 06:22:27,044 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 20)
2014-07-23 06:22:27,044 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 20 in 1310 ms on dco-node130-mgt.dco.ethz.ch (progress: 3/81)
2014-07-23 06:22:27,291 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:23 as TID 23 on executor 9: dco-node121-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:27,292 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:23 as 2027 bytes in 1 ms
2014-07-23 06:22:27,301 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 21)
2014-07-23 06:22:27,301 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 21 in 1504 ms on dco-node121-mgt.dco.ethz.ch (progress: 4/81)
2014-07-23 06:22:28,313 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:24 as TID 24 on executor 13: dco-node130-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:28,314 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:24 as 2027 bytes in 1 ms
2014-07-23 06:22:28,319 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 22)
2014-07-23 06:22:28,319 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 22 in 1282 ms on dco-node130-mgt.dco.ethz.ch (progress: 5/81)
2014-07-23 06:22:28,686 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:25 as TID 25 on executor 7: dco-node133-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:28,687 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:25 as 2027 bytes in 0 ms
2014-07-23 06:22:28,694 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 19)
2014-07-23 06:22:28,695 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 19 in 6124 ms on dco-node133-mgt.dco.ethz.ch (progress: 6/81)
2014-07-23 06:22:28,722 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:26 as TID 26 on executor 9: dco-node121-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:28,723 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:26 as 2027 bytes in 1 ms
2014-07-23 06:22:28,730 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 23)
2014-07-23 06:22:28,730 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 23 in 1432 ms on dco-node121-mgt.dco.ethz.ch (progress: 7/81)
2014-07-23 06:22:29,166 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:27 as TID 27 on executor 10: dco-node129-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:29,167 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:27 as 2027 bytes in 0 ms
2014-07-23 06:22:29,172 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 1 in 7126 ms on dco-node129-mgt.dco.ethz.ch (progress: 8/81)
2014-07-23 06:22:29,172 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 1)
2014-07-23 06:22:29,175 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 5)
2014-07-23 06:22:29,175 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 5 in 7102 ms on dco-node127-mgt.dco.ethz.ch (progress: 9/81)
2014-07-23 06:22:29,176 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:28 as TID 28 on executor 8: dco-node127-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:29,176 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:28 as 2027 bytes in 0 ms
2014-07-23 06:22:29,604 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:29 as TID 29 on executor 13: dco-node130-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:29,605 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:29 as 2027 bytes in 1 ms
2014-07-23 06:22:29,609 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 24)
2014-07-23 06:22:29,609 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 24 in 1293 ms on dco-node130-mgt.dco.ethz.ch (progress: 10/81)
2014-07-23 06:22:29,806 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:30 as TID 30 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:29,807 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:30 as 2027 bytes in 0 ms
2014-07-23 06:22:29,811 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 2)
2014-07-23 06:22:29,811 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 2 in 7747 ms on dco-node123-mgt.dco.ethz.ch (progress: 11/81)
2014-07-23 06:22:29,847 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:31 as TID 31 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:29,848 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:31 as 2027 bytes in 1 ms
2014-07-23 06:22:29,854 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 3)
2014-07-23 06:22:29,854 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 3 in 7782 ms on dco-node123-mgt.dco.ethz.ch (progress: 12/81)
2014-07-23 06:22:29,966 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:32 as TID 32 on executor 7: dco-node133-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:29,967 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:32 as 2027 bytes in 1 ms
2014-07-23 06:22:29,972 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 25)
2014-07-23 06:22:29,972 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 25 in 1282 ms on dco-node133-mgt.dco.ethz.ch (progress: 13/81)
2014-07-23 06:22:30,039 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:33 as TID 33 on executor 14: dco-node136-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:30,040 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:33 as 2027 bytes in 1 ms
2014-07-23 06:22:30,045 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 9)
2014-07-23 06:22:30,045 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 9 in 7947 ms on dco-node136-mgt.dco.ethz.ch (progress: 14/81)
2014-07-23 06:22:30,049 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:34 as TID 34 on executor 9: dco-node121-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:30,049 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:34 as 2027 bytes in 0 ms
2014-07-23 06:22:30,054 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 26)
2014-07-23 06:22:30,055 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 26 in 1327 ms on dco-node121-mgt.dco.ethz.ch (progress: 15/81)
2014-07-23 06:22:30,246 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:35 as TID 35 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:30,247 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:35 as 2027 bytes in 1 ms
2014-07-23 06:22:30,252 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 14)
2014-07-23 06:22:30,252 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 14 in 8131 ms on dco-node124-mgt.dco.ethz.ch (progress: 16/81)
2014-07-23 06:22:30,253 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:36 as TID 36 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:30,253 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:36 as 2027 bytes in 0 ms
2014-07-23 06:22:30,257 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 13 in 8139 ms on dco-node124-mgt.dco.ethz.ch (progress: 17/81)
2014-07-23 06:22:30,257 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 13)
2014-07-23 06:22:30,425 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:37 as TID 37 on executor 8: dco-node127-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:30,425 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:37 as 2027 bytes in 0 ms
2014-07-23 06:22:30,430 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 28)
2014-07-23 06:22:30,430 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 28 in 1251 ms on dco-node127-mgt.dco.ethz.ch (progress: 18/81)
2014-07-23 06:22:30,434 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:38 as TID 38 on executor 10: dco-node129-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:30,434 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:38 as 2027 bytes in 0 ms
2014-07-23 06:22:30,439 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 27)
2014-07-23 06:22:30,439 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 27 in 1269 ms on dco-node129-mgt.dco.ethz.ch (progress: 19/81)
2014-07-23 06:22:30,475 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:39 as TID 39 on executor 15: dco-node126-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:30,476 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:39 as 2027 bytes in 0 ms
2014-07-23 06:22:30,481 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 8)
2014-07-23 06:22:30,481 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 8 in 8385 ms on dco-node126-mgt.dco.ethz.ch (progress: 20/81)
2014-07-23 06:22:30,919 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:40 as TID 40 on executor 5: dco-node132-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:30,919 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:40 as 2027 bytes in 0 ms
2014-07-23 06:22:30,924 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 4)
2014-07-23 06:22:30,924 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 4 in 8851 ms on dco-node132-mgt.dco.ethz.ch (progress: 21/81)
2014-07-23 06:22:30,949 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:41 as TID 41 on executor 12: dco-node134-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:30,949 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:41 as 2027 bytes in 0 ms
2014-07-23 06:22:30,954 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 16)
2014-07-23 06:22:30,954 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 16 in 8707 ms on dco-node134-mgt.dco.ethz.ch (progress: 22/81)
2014-07-23 06:22:31,084 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:42 as TID 42 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,085 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:42 as 2027 bytes in 1 ms
2014-07-23 06:22:31,089 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 30 in 1279 ms on dco-node123-mgt.dco.ethz.ch (progress: 23/81)
2014-07-23 06:22:31,089 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 30)
2014-07-23 06:22:31,095 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:43 as TID 43 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,095 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:43 as 2027 bytes in 0 ms
2014-07-23 06:22:31,099 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 31 in 1249 ms on dco-node123-mgt.dco.ethz.ch (progress: 24/81)
2014-07-23 06:22:31,099 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 31)
2014-07-23 06:22:31,199 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:44 as TID 44 on executor 13: dco-node130-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,200 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:44 as 2027 bytes in 1 ms
2014-07-23 06:22:31,206 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 29)
2014-07-23 06:22:31,206 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 29 in 1596 ms on dco-node130-mgt.dco.ethz.ch (progress: 25/81)
2014-07-23 06:22:31,230 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:45 as TID 45 on executor 7: dco-node133-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,231 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:45 as 2027 bytes in 1 ms
2014-07-23 06:22:31,234 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 32 in 1265 ms on dco-node133-mgt.dco.ethz.ch (progress: 26/81)
2014-07-23 06:22:31,234 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 32)
2014-07-23 06:22:31,307 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:46 as TID 46 on executor 14: dco-node136-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,308 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:46 as 2027 bytes in 1 ms
2014-07-23 06:22:31,312 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 33)
2014-07-23 06:22:31,312 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 33 in 1269 ms on dco-node136-mgt.dco.ethz.ch (progress: 27/81)
2014-07-23 06:22:31,364 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:47 as TID 47 on executor 0: dco-node135-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,364 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:47 as 2027 bytes in 0 ms
2014-07-23 06:22:31,367 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 17 in 9016 ms on dco-node135-mgt.dco.ethz.ch (progress: 28/81)
2014-07-23 06:22:31,367 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 17)
2014-07-23 06:22:31,367 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:48 as TID 48 on executor 0: dco-node135-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,368 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:48 as 2027 bytes in 0 ms
2014-07-23 06:22:31,372 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 18)
2014-07-23 06:22:31,372 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 18 in 9018 ms on dco-node135-mgt.dco.ethz.ch (progress: 29/81)
2014-07-23 06:22:31,521 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:49 as TID 49 on executor 9: dco-node121-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,522 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:49 as 2027 bytes in 0 ms
2014-07-23 06:22:31,526 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 34)
2014-07-23 06:22:31,526 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 34 in 1474 ms on dco-node121-mgt.dco.ethz.ch (progress: 30/81)
2014-07-23 06:22:31,527 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:50 as TID 50 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,528 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:50 as 2027 bytes in 0 ms
2014-07-23 06:22:31,528 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:51 as TID 51 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,529 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:51 as 2027 bytes in 1 ms
2014-07-23 06:22:31,533 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 35)
2014-07-23 06:22:31,533 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 35 in 1283 ms on dco-node124-mgt.dco.ethz.ch (progress: 31/81)
2014-07-23 06:22:31,537 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 36)
2014-07-23 06:22:31,537 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 36 in 1281 ms on dco-node124-mgt.dco.ethz.ch (progress: 32/81)
2014-07-23 06:22:31,642 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:52 as TID 52 on executor 10: dco-node129-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,643 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:52 as 2027 bytes in 1 ms
2014-07-23 06:22:31,647 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 38)
2014-07-23 06:22:31,647 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 38 in 1209 ms on dco-node129-mgt.dco.ethz.ch (progress: 33/81)
2014-07-23 06:22:31,651 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:53 as TID 53 on executor 8: dco-node127-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,652 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:53 as 2027 bytes in 1 ms
2014-07-23 06:22:31,656 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 37)
2014-07-23 06:22:31,656 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 37 in 1227 ms on dco-node127-mgt.dco.ethz.ch (progress: 34/81)
2014-07-23 06:22:31,704 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:54 as TID 54 on executor 15: dco-node126-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:31,704 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:54 as 2027 bytes in 0 ms
2014-07-23 06:22:31,708 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 39 in 1229 ms on dco-node126-mgt.dco.ethz.ch (progress: 35/81)
2014-07-23 06:22:31,708 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 39)
2014-07-23 06:22:32,158 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:55 as TID 55 on executor 6: dco-node131-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,159 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:55 as 2027 bytes in 1 ms
2014-07-23 06:22:32,163 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 15 in 10038 ms on dco-node131-mgt.dco.ethz.ch (progress: 36/81)
2014-07-23 06:22:32,163 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 15)
2014-07-23 06:22:32,168 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:56 as TID 56 on executor 11: dco-node128-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,168 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:56 as 2027 bytes in 0 ms
2014-07-23 06:22:32,172 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 10 in 10066 ms on dco-node128-mgt.dco.ethz.ch (progress: 37/81)
2014-07-23 06:22:32,172 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 10)
2014-07-23 06:22:32,189 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:57 as TID 57 on executor 5: dco-node132-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,189 [spark-akka.actor.default-dispatcher-16] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:57 as 2027 bytes in 0 ms
2014-07-23 06:22:32,193 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 40 in 1271 ms on dco-node132-mgt.dco.ethz.ch (progress: 38/81)
2014-07-23 06:22:32,193 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 40)
2014-07-23 06:22:32,212 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:58 as TID 58 on executor 12: dco-node134-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,213 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:58 as 2027 bytes in 0 ms
2014-07-23 06:22:32,217 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 41)
2014-07-23 06:22:32,217 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 41 in 1265 ms on dco-node134-mgt.dco.ethz.ch (progress: 39/81)
2014-07-23 06:22:32,262 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:59 as TID 59 on executor 4: dco-node122-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,263 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:59 as 2027 bytes in 1 ms
2014-07-23 06:22:32,267 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 11)
2014-07-23 06:22:32,267 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 11 in 10155 ms on dco-node122-mgt.dco.ethz.ch (progress: 40/81)
2014-07-23 06:22:32,460 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:60 as TID 60 on executor 1: dco-node125-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,461 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:60 as 2027 bytes in 1 ms
2014-07-23 06:22:32,464 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 7 in 10380 ms on dco-node125-mgt.dco.ethz.ch (progress: 41/81)
2014-07-23 06:22:32,464 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 7)
2014-07-23 06:22:32,522 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:61 as TID 61 on executor 1: dco-node125-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,523 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:61 as 2027 bytes in 1 ms
2014-07-23 06:22:32,526 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 6 in 10443 ms on dco-node125-mgt.dco.ethz.ch (progress: 42/81)
2014-07-23 06:22:32,526 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 6)
2014-07-23 06:22:32,534 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:62 as TID 62 on executor 7: dco-node133-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,535 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:62 as 2027 bytes in 1 ms
2014-07-23 06:22:32,540 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 45 in 1305 ms on dco-node133-mgt.dco.ethz.ch (progress: 43/81)
2014-07-23 06:22:32,540 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 45)
2014-07-23 06:22:32,541 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:63 as TID 63 on executor 14: dco-node136-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,542 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:63 as 2027 bytes in 1 ms
2014-07-23 06:22:32,546 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 46)
2014-07-23 06:22:32,546 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 46 in 1235 ms on dco-node136-mgt.dco.ethz.ch (progress: 44/81)
2014-07-23 06:22:32,571 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:64 as TID 64 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,572 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:64 as 2027 bytes in 0 ms
2014-07-23 06:22:32,574 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 43 in 1477 ms on dco-node123-mgt.dco.ethz.ch (progress: 45/81)
2014-07-23 06:22:32,574 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 43)
2014-07-23 06:22:32,582 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:65 as TID 65 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,582 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:65 as 2027 bytes in 0 ms
2014-07-23 06:22:32,586 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 42)
2014-07-23 06:22:32,586 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 42 in 1499 ms on dco-node123-mgt.dco.ethz.ch (progress: 46/81)
2014-07-23 06:22:32,650 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:66 as TID 66 on executor 0: dco-node135-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,651 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:66 as 2027 bytes in 1 ms
2014-07-23 06:22:32,656 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 48 in 1284 ms on dco-node135-mgt.dco.ethz.ch (progress: 47/81)
2014-07-23 06:22:32,656 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 48)
2014-07-23 06:22:32,678 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:67 as TID 67 on executor 0: dco-node135-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,679 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:67 as 2027 bytes in 1 ms
2014-07-23 06:22:32,683 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 47 in 1316 ms on dco-node135-mgt.dco.ethz.ch (progress: 48/81)
2014-07-23 06:22:32,683 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 47)
2014-07-23 06:22:32,734 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:68 as TID 68 on executor 13: dco-node130-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,735 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:68 as 2027 bytes in 1 ms
2014-07-23 06:22:32,739 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 44 in 1536 ms on dco-node130-mgt.dco.ethz.ch (progress: 49/81)
2014-07-23 06:22:32,739 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 44)
2014-07-23 06:22:32,857 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:69 as TID 69 on executor 10: dco-node129-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,857 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:69 as 2027 bytes in 0 ms
2014-07-23 06:22:32,860 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 52 in 1215 ms on dco-node129-mgt.dco.ethz.ch (progress: 50/81)
2014-07-23 06:22:32,861 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 52)
2014-07-23 06:22:32,865 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:70 as TID 70 on executor 8: dco-node127-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,866 [spark-akka.actor.default-dispatcher-20] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:70 as 2027 bytes in 1 ms
2014-07-23 06:22:32,871 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 53 in 1215 ms on dco-node127-mgt.dco.ethz.ch (progress: 51/81)
2014-07-23 06:22:32,871 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 53)
2014-07-23 06:22:32,922 [spark-akka.actor.default-dispatcher-19] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:71 as TID 71 on executor 15: dco-node126-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,922 [spark-akka.actor.default-dispatcher-19] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:71 as 2027 bytes in 0 ms
2014-07-23 06:22:32,927 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 54 in 1219 ms on dco-node126-mgt.dco.ethz.ch (progress: 52/81)
2014-07-23 06:22:32,927 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 54)
2014-07-23 06:22:32,928 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:72 as TID 72 on executor 9: dco-node121-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:32,928 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:72 as 2027 bytes in 0 ms
2014-07-23 06:22:32,930 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 49 in 1407 ms on dco-node121-mgt.dco.ethz.ch (progress: 53/81)
2014-07-23 06:22:32,930 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 49)
2014-07-23 06:22:32,999 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:73 as TID 73 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:33,000 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:73 as 2027 bytes in 0 ms
2014-07-23 06:22:33,004 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 51 in 1472 ms on dco-node124-mgt.dco.ethz.ch (progress: 54/81)
2014-07-23 06:22:33,004 [spark-akka.actor.default-dispatcher-6] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 51)
2014-07-23 06:22:33,025 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:74 as TID 74 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:33,026 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:74 as 2027 bytes in 1 ms
2014-07-23 06:22:33,030 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 50)
2014-07-23 06:22:33,030 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 50 in 1499 ms on dco-node124-mgt.dco.ethz.ch (progress: 55/81)
2014-07-23 06:22:33,390 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:75 as TID 75 on executor 5: dco-node132-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:33,390 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:75 as 2027 bytes in 0 ms
2014-07-23 06:22:33,394 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 57 in 1202 ms on dco-node132-mgt.dco.ethz.ch (progress: 56/81)
2014-07-23 06:22:33,394 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 57)
2014-07-23 06:22:33,417 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:76 as TID 76 on executor 12: dco-node134-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:33,418 [spark-akka.actor.default-dispatcher-5] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:76 as 2027 bytes in 1 ms
2014-07-23 06:22:33,421 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 58)
2014-07-23 06:22:33,421 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 58 in 1206 ms on dco-node134-mgt.dco.ethz.ch (progress: 57/81)
2014-07-23 06:22:33,421 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:77 as TID 77 on executor 11: dco-node128-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:33,422 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:77 as 2027 bytes in 1 ms
2014-07-23 06:22:33,425 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 56 in 1254 ms on dco-node128-mgt.dco.ethz.ch (progress: 58/81)
2014-07-23 06:22:33,425 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 56)
2014-07-23 06:22:33,450 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:78 as TID 78 on executor 6: dco-node131-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:33,451 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:78 as 2027 bytes in 1 ms
2014-07-23 06:22:33,453 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 55 in 1293 ms on dco-node131-mgt.dco.ethz.ch (progress: 59/81)
2014-07-23 06:22:33,453 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 55)
2014-07-23 06:22:33,579 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:79 as TID 79 on executor 4: dco-node122-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:33,579 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:79 as 2027 bytes in 0 ms
2014-07-23 06:22:33,583 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 59)
2014-07-23 06:22:33,583 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 59 in 1318 ms on dco-node122-mgt.dco.ethz.ch (progress: 60/81)
2014-07-23 06:22:33,761 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0:80 as TID 80 on executor 14: dco-node136-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:33,779 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 0.0:80 as 2027 bytes in 0 ms
2014-07-23 06:22:33,782 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 63 in 1239 ms on dco-node136-mgt.dco.ethz.ch (progress: 61/81)
2014-07-23 06:22:33,782 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 63)
2014-07-23 06:22:33,850 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 60)
2014-07-23 06:22:33,851 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 60 in 1388 ms on dco-node125-mgt.dco.ethz.ch (progress: 62/81)
2014-07-23 06:22:33,876 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 61)
2014-07-23 06:22:33,876 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 61 in 1351 ms on dco-node125-mgt.dco.ethz.ch (progress: 63/81)
2014-07-23 06:22:33,975 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 64 in 1401 ms on dco-node123-mgt.dco.ethz.ch (progress: 64/81)
2014-07-23 06:22:33,975 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 64)
2014-07-23 06:22:34,006 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 65)
2014-07-23 06:22:34,006 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 65 in 1422 ms on dco-node123-mgt.dco.ethz.ch (progress: 65/81)
2014-07-23 06:22:34,029 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 62)
2014-07-23 06:22:34,029 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 62 in 1493 ms on dco-node133-mgt.dco.ethz.ch (progress: 66/81)
2014-07-23 06:22:34,118 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 66 in 1466 ms on dco-node135-mgt.dco.ethz.ch (progress: 67/81)
2014-07-23 06:22:34,118 [spark-akka.actor.default-dispatcher-19] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 66)
2014-07-23 06:22:34,136 [spark-akka.actor.default-dispatcher-19] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 67)
2014-07-23 06:22:34,136 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 67 in 1456 ms on dco-node135-mgt.dco.ethz.ch (progress: 68/81)
2014-07-23 06:22:34,185 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 71 in 1261 ms on dco-node126-mgt.dco.ethz.ch (progress: 69/81)
2014-07-23 06:22:34,185 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 71)
2014-07-23 06:22:34,268 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 68 in 1532 ms on dco-node130-mgt.dco.ethz.ch (progress: 70/81)
2014-07-23 06:22:34,268 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 68)
2014-07-23 06:22:34,281 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 70 in 1414 ms on dco-node127-mgt.dco.ethz.ch (progress: 71/81)
2014-07-23 06:22:34,281 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 70)
2014-07-23 06:22:34,331 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 72 in 1401 ms on dco-node121-mgt.dco.ethz.ch (progress: 72/81)
2014-07-23 06:22:34,331 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 72)
2014-07-23 06:22:34,345 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 69 in 1486 ms on dco-node129-mgt.dco.ethz.ch (progress: 73/81)
2014-07-23 06:22:34,345 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 69)
2014-07-23 06:22:34,408 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 73 in 1406 ms on dco-node124-mgt.dco.ethz.ch (progress: 74/81)
2014-07-23 06:22:34,408 [spark-akka.actor.default-dispatcher-18] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 73)
2014-07-23 06:22:34,441 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 74 in 1414 ms on dco-node124-mgt.dco.ethz.ch (progress: 75/81)
2014-07-23 06:22:34,441 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 74)
2014-07-23 06:22:34,630 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 76 in 1211 ms on dco-node134-mgt.dco.ethz.ch (progress: 76/81)
2014-07-23 06:22:34,630 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 76)
2014-07-23 06:22:34,638 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 77 in 1215 ms on dco-node128-mgt.dco.ethz.ch (progress: 77/81)
2014-07-23 06:22:34,638 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 77)
2014-07-23 06:22:34,662 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 75 in 1271 ms on dco-node132-mgt.dco.ethz.ch (progress: 78/81)
2014-07-23 06:22:34,662 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 75)
2014-07-23 06:22:34,700 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 78 in 1248 ms on dco-node131-mgt.dco.ethz.ch (progress: 79/81)
2014-07-23 06:22:34,700 [spark-akka.actor.default-dispatcher-4] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 78)
2014-07-23 06:22:34,791 [spark-akka.actor.default-dispatcher-21] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 79)
2014-07-23 06:22:34,791 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 79 in 1209 ms on dco-node122-mgt.dco.ethz.ch (progress: 80/81)
2014-07-23 06:22:35,232 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 80 in 1467 ms on dco-node136-mgt.dco.ethz.ch (progress: 81/81)
2014-07-23 06:22:35,232 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(0, 80)
2014-07-23 06:22:35,233 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2014-07-23 06:22:35,234 [spark-akka.actor.default-dispatcher-14] INFO  org.apache.spark.scheduler.DAGScheduler - Stage 0 (collect at RemoveInfrequentWordsApp.scala:51) finished in 1197.525 s
2014-07-23 06:22:35,247 [main] INFO  org.apache.spark.SparkContext - Job finished: collect at RemoveInfrequentWordsApp.scala:51, took 1197.688213996 s
2014-07-23 06:22:35,509 [main] INFO  org.apache.spark.SparkContext - Starting job: foreach at RemoveInfrequentWordsApp.scala:27
2014-07-23 06:22:35,510 [spark-akka.actor.default-dispatcher-21] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (foreach at RemoveInfrequentWordsApp.scala:27) with 20 output partitions (allowLocal=false)
2014-07-23 06:22:35,510 [spark-akka.actor.default-dispatcher-21] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 1(foreach at RemoveInfrequentWordsApp.scala:27)
2014-07-23 06:22:35,510 [spark-akka.actor.default-dispatcher-21] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2014-07-23 06:22:35,514 [spark-akka.actor.default-dispatcher-21] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2014-07-23 06:22:35,514 [spark-akka.actor.default-dispatcher-21] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (ParallelCollectionRDD[6] at parallelize at RemoveInfrequentWordsApp.scala:27), which has no missing parents
2014-07-23 06:22:35,641 [spark-akka.actor.default-dispatcher-21] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 20 missing tasks from Stage 1 (ParallelCollectionRDD[6] at parallelize at RemoveInfrequentWordsApp.scala:27)
2014-07-23 06:22:35,641 [spark-akka.actor.default-dispatcher-21] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 20 tasks
2014-07-23 06:22:35,642 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:0 as TID 81 on executor 11: dco-node128-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,646 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:0 as 75805 bytes in 4 ms
2014-07-23 06:22:35,646 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:1 as TID 82 on executor 6: dco-node131-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,649 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:1 as 75805 bytes in 3 ms
2014-07-23 06:22:35,649 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:2 as TID 83 on executor 15: dco-node126-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,652 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:2 as 75805 bytes in 3 ms
2014-07-23 06:22:35,652 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:3 as TID 84 on executor 9: dco-node121-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,654 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:3 as 75805 bytes in 2 ms
2014-07-23 06:22:35,654 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:4 as TID 85 on executor 4: dco-node122-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,656 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:4 as 75805 bytes in 2 ms
2014-07-23 06:22:35,656 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:5 as TID 86 on executor 14: dco-node136-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,659 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:5 as 75805 bytes in 3 ms
2014-07-23 06:22:35,660 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:6 as TID 87 on executor 12: dco-node134-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,663 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:6 as 75805 bytes in 2 ms
2014-07-23 06:22:35,663 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:7 as TID 88 on executor 7: dco-node133-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,665 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:7 as 75805 bytes in 2 ms
2014-07-23 06:22:35,665 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:8 as TID 89 on executor 10: dco-node129-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,676 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:8 as 75805 bytes in 11 ms
2014-07-23 06:22:35,676 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:9 as TID 90 on executor 1: dco-node125-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,679 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:9 as 75805 bytes in 3 ms
2014-07-23 06:22:35,679 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:10 as TID 91 on executor 13: dco-node130-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,681 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:10 as 75805 bytes in 2 ms
2014-07-23 06:22:35,681 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:11 as TID 92 on executor 5: dco-node132-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,683 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:11 as 75805 bytes in 2 ms
2014-07-23 06:22:35,683 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:12 as TID 93 on executor 0: dco-node135-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,685 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:12 as 75805 bytes in 2 ms
2014-07-23 06:22:35,685 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:13 as TID 94 on executor 8: dco-node127-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,688 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:13 as 75805 bytes in 3 ms
2014-07-23 06:22:35,688 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:14 as TID 95 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,691 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:14 as 75805 bytes in 3 ms
2014-07-23 06:22:35,691 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:15 as TID 96 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,693 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:15 as 75805 bytes in 2 ms
2014-07-23 06:22:35,693 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:16 as TID 97 on executor 1: dco-node125-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,695 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:16 as 75805 bytes in 2 ms
2014-07-23 06:22:35,696 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:17 as TID 98 on executor 0: dco-node135-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,698 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:17 as 75805 bytes in 1 ms
2014-07-23 06:22:35,698 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:18 as TID 99 on executor 3: dco-node123-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,699 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:18 as 75805 bytes in 1 ms
2014-07-23 06:22:35,700 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0:19 as TID 100 on executor 2: dco-node124-mgt.dco.ethz.ch (PROCESS_LOCAL)
2014-07-23 06:22:35,701 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.TaskSetManager - Serialized task 1.0:19 as 75805 bytes in 1 ms
2014-07-23 06:24:11,814 [spark-akka.actor.default-dispatcher-34] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(1, 5)
2014-07-23 06:24:11,814 [Result resolver thread-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 86 in 96155 ms on dco-node136-mgt.dco.ethz.ch (progress: 1/20)
2014-07-23 06:24:18,062 [spark-akka.actor.default-dispatcher-15] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(1, 6)
2014-07-23 06:24:18,062 [Result resolver thread-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 87 in 102400 ms on dco-node134-mgt.dco.ethz.ch (progress: 2/20)
2014-07-23 06:24:33,207 [spark-akka.actor.default-dispatcher-17] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(1, 13)
2014-07-23 06:24:33,207 [Result resolver thread-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 94 in 117519 ms on dco-node127-mgt.dco.ethz.ch (progress: 3/20)
2014-07-23 06:24:38,628 [Result resolver thread-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished TID 88 in 122962 ms on dco-node133-mgt.dco.ethz.ch (progress: 4/20)
2014-07-23 06:24:38,628 [spark-akka.actor.default-dispatcher-35] INFO  org.apache.spark.scheduler.DAGScheduler - Completed ResultTask(1, 7)
