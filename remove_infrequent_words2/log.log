Spark assembly has been built with Hive, including Datanucleus jars on classpath
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0
=====================================
Remove Infrequent Words Configuration
-------------------------------------
maxWordCount=2147483647, minWordCount=500, output=hdfs://dco-node121.dco.ethz.ch:54310/cw-combined-pruned-500, inputCombined=hdfs://dco-node121.dco.ethz.ch:54310/cw-combined, inputWordcount=hdfs://dco-node121.dco.ethz.ch:54310/cw-wordcount/wordcounts.txt
=====================================
2014-07-23 05:36:59,300 [main] WARN  org.apache.spark.SparkConf - 
SPARK_JAVA_OPTS was detected (set to '-Xms4g -Xmx70g  -Dspark.akka.frameSize=2000').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
2014-07-23 05:36:59,302 [main] WARN  org.apache.spark.SparkConf - Setting 'spark.executor.extraJavaOptions' to '-Xms4g -Xmx70g  -Dspark.akka.frameSize=2000' as a work-around.
2014-07-23 05:36:59,302 [main] WARN  org.apache.spark.SparkConf - Setting 'spark.driver.extraJavaOptions' to '-Xms4g -Xmx70g  -Dspark.akka.frameSize=2000' as a work-around.
2014-07-23 05:36:59,359 [main] INFO  org.apache.spark.SecurityManager - Changing view acls to: root
2014-07-23 05:36:59,359 [main] INFO  org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root)
2014-07-23 05:36:59,795 [spark-akka.actor.default-dispatcher-4] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started
2014-07-23 05:36:59,853 [spark-akka.actor.default-dispatcher-2] INFO  Remoting - Starting remoting
2014-07-23 05:37:00,023 [spark-akka.actor.default-dispatcher-2] INFO  Remoting - Remoting started; listening on addresses :[akka.tcp://spark@dco-node121-mgt.dco.ethz.ch:42121]
2014-07-23 05:37:00,028 [spark-akka.actor.default-dispatcher-4] INFO  Remoting - Remoting now listens on addresses: [akka.tcp://spark@dco-node121-mgt.dco.ethz.ch:42121]
2014-07-23 05:37:00,058 [main] INFO  org.apache.spark.SparkEnv - Registering MapOutputTracker
2014-07-23 05:37:00,061 [main] INFO  org.apache.spark.SparkEnv - Registering BlockManagerMaster
2014-07-23 05:37:00,077 [main] INFO  org.apache.spark.storage.DiskBlockManager - Created local directory at /tmp/spark-local-20140723053700-3ebe
2014-07-23 05:37:00,082 [main] INFO  org.apache.spark.storage.MemoryStore - MemoryStore started with capacity 40.3 GB.
2014-07-23 05:37:00,111 [main] INFO  org.apache.spark.network.ConnectionManager - Bound socket to port 48623 with id = ConnectionManagerId(dco-node121-mgt.dco.ethz.ch,48623)
2014-07-23 05:37:00,116 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Trying to register BlockManager
2014-07-23 05:37:00,119 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.storage.BlockManagerInfo - Registering block manager dco-node121-mgt.dco.ethz.ch:48623 with 40.3 GB RAM
2014-07-23 05:37:00,120 [main] INFO  org.apache.spark.storage.BlockManagerMaster - Registered BlockManager
2014-07-23 05:37:00,135 [main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2014-07-23 05:37:00,195 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 05:37:00,215 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:47287
2014-07-23 05:37:00,216 [main] INFO  org.apache.spark.broadcast.HttpBroadcast - Broadcast server started at http://172.31.109.131:47287
2014-07-23 05:37:00,222 [main] INFO  org.apache.spark.HttpFileServer - HTTP File server directory is /tmp/spark-e8a5ecaa-3e5b-4edf-8ba8-cb7ef86f556d
2014-07-23 05:37:00,222 [main] INFO  org.apache.spark.HttpServer - Starting HTTP Server
2014-07-23 05:37:00,223 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 05:37:00,226 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SocketConnector@0.0.0.0:39872
2014-07-23 05:37:00,581 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.y.z-SNAPSHOT
2014-07-23 05:37:00,597 [main] INFO  org.eclipse.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:4040
2014-07-23 05:37:00,605 [main] INFO  org.apache.spark.ui.SparkUI - Started SparkUI at http://dco-node121-mgt.dco.ethz.ch:4040
2014-07-23 05:37:00,827 [main] WARN  org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2014-07-23 05:37:01,498 [main] INFO  org.apache.spark.scheduler.EventLoggingListener - Logging events to hdfs://dco-node121.dco.ethz.ch:54310/spark_event_log/remove-infrequent-words-1406086620924
2014-07-23 05:37:01,745 [main] INFO  org.apache.spark.SparkContext - Added JAR file:/disk3/user_work/runs/remove_infrequent_words2/run.jar at http://172.31.109.131:39872/jars/run.jar with timestamp 1406086621744
2014-07-23 05:37:01,811 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.deploy.client.AppClient$ClientActor - Connecting to master spark://dco-node121.dco.ethz.ch:7077...
2014-07-23 05:37:02,070 [main] INFO  org.apache.spark.storage.MemoryStore - ensureFreeSpace(231905) called with curMem=0, maxMem=43218213273
2014-07-23 05:37:02,072 [main] INFO  org.apache.spark.storage.MemoryStore - Block broadcast_0 stored as values to memory (estimated size 226.5 KB, free 40.2 GB)
2014-07-23 05:37:02,173 [spark-akka.actor.default-dispatcher-3] INFO  org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend - Connected to Spark cluster with app ID app-20140723053702-0011
2014-07-23 05:37:02,364 [main] INFO  org.apache.hadoop.mapred.FileInputFormat - Total input paths to process : 1
2014-07-23 05:37:02,407 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.133:50010
2014-07-23 05:37:02,408 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.135:50010
2014-07-23 05:37:02,408 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.146:50010
2014-07-23 05:37:02,409 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.139:50010
2014-07-23 05:37:02,409 [main] INFO  org.apache.hadoop.net.NetworkTopology - Adding a new node: /default-rack/172.31.109.142:50010
2014-07-23 05:37:02,416 [main] INFO  org.apache.spark.SparkContext - Starting job: take at RemoveInfrequentWordsApp.scala:46
2014-07-23 05:37:02,432 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (take at RemoveInfrequentWordsApp.scala:46) with 1 output partitions (allowLocal=true)
2014-07-23 05:37:02,433 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 0(take at RemoveInfrequentWordsApp.scala:46)
2014-07-23 05:37:02,433 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2014-07-23 05:37:02,450 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2014-07-23 05:37:02,451 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Computing the requested partition locally
2014-07-23 05:37:02,460 [Local computation of job 0] INFO  org.apache.spark.rdd.HadoopRDD - Input split: hdfs://dco-node121.dco.ethz.ch:54310/cw-wordcount/wordcounts.txt:0+16777216
2014-07-23 05:37:02,465 [Local computation of job 0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2014-07-23 05:37:02,466 [Local computation of job 0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2014-07-23 05:37:02,466 [Local computation of job 0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2014-07-23 05:37:02,466 [Local computation of job 0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2014-07-23 05:37:02,466 [Local computation of job 0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.id is deprecated. Instead, use mapreduce.job.id
2014-07-23 05:37:02,525 [main] INFO  org.apache.spark.SparkContext - Job finished: take at RemoveInfrequentWordsApp.scala:46, took 0.108430461 s
2014-07-23 05:37:02,773 [main] INFO  org.apache.spark.SparkContext - Starting job: foreach at RemoveInfrequentWordsApp.scala:27
2014-07-23 05:37:02,775 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (foreach at RemoveInfrequentWordsApp.scala:27) with 20 output partitions (allowLocal=false)
2014-07-23 05:37:02,775 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: Stage 1(foreach at RemoveInfrequentWordsApp.scala:27)
2014-07-23 05:37:02,775 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
2014-07-23 05:37:02,778 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
2014-07-23 05:37:02,782 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting Stage 1 (ParallelCollectionRDD[2] at parallelize at RemoveInfrequentWordsApp.scala:27), which has no missing parents
2014-07-23 05:37:02,842 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 20 missing tasks from Stage 1 (ParallelCollectionRDD[2] at parallelize at RemoveInfrequentWordsApp.scala:27)
2014-07-23 05:37:02,844 [spark-akka.actor.default-dispatcher-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 20 tasks
2014-07-23 05:37:17,853 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 05:37:32,853 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 05:37:47,852 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 05:38:02,852 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 05:38:17,853 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 05:38:32,852 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 05:38:47,852 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
2014-07-23 05:39:02,852 [Timer-0] WARN  org.apache.spark.scheduler.TaskSchedulerImpl - Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
